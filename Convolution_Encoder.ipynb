{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Convolution_Encoder",
      "provenance": [],
      "authorship_tag": "ABX9TyP0GSWGQsvrQpO00jRmYzeV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShwetaNikam15/NNDL_2022/blob/main/Convolution_Encoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Autoencoder feature extraction method for classification**\n",
        "\n",
        "Aim: To use autoencoder feature extraction method for classification\n",
        "\n",
        "Theory:\n",
        "\n",
        "An autoencoder is a special type of neural network that is trained to copy its input to its output. For example, given an image of a handwritten digit, an autoencoder first encodes the image into a lower dimensional latent representation, then decodes the latent representation back to an image. Data specific means that the autoencoder will only be able to actually compress the data on which it has been trained. For example, if you train an autoencoder with images of dogs, then it will give a bad performance for cats. The autoencoder plans to learn the representation which is known as the encoding for a whole set of data. This can result in the reduction of the dimensionality by the training network. The reconstruction part is also learned with this.\n",
        "\n",
        "The Input will be passed through a layer of encoders which are actually a fully connected neural network that also makes the code decoder and hence use the same code for encoding and decoding like an ANN."
      ],
      "metadata": {
        "id": "tVe3oA179XQ9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30Zos8XV8muv",
        "outputId": "4002e749-398d-4ae9-f651-a63354bc341a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " inputs (InputLayer)         [(None, 28, 28, 1)]       0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 28, 28, 32)        320       \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 28, 28, 32)       128       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " leaky_re_lu (LeakyReLU)     (None, 28, 28, 32)        0         \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 14, 14, 32)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 14, 14, 64)        18496     \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 14, 14, 64)       256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " leaky_re_lu_1 (LeakyReLU)   (None, 14, 14, 64)        0         \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 7, 7, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 3136)              0         \n",
            "                                                                 \n",
            " latent (Dense)              (None, 128)               401536    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 3136)              404544    \n",
            "                                                                 \n",
            " leaky_re_lu_2 (LeakyReLU)   (None, 3136)              0         \n",
            "                                                                 \n",
            " reshape (Reshape)           (None, 7, 7, 64)          0         \n",
            "                                                                 \n",
            " conv2d_transpose (Conv2DTra  (None, 14, 14, 64)       36928     \n",
            " nspose)                                                         \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 14, 14, 64)       256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " leaky_re_lu_3 (LeakyReLU)   (None, 14, 14, 64)        0         \n",
            "                                                                 \n",
            " conv2d_transpose_1 (Conv2DT  (None, 28, 28, 1)        577       \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 28, 28, 1)        4         \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " outputs (Activation)        (None, 28, 28, 1)         0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 863,045\n",
            "Trainable params: 862,723\n",
            "Non-trainable params: 322\n",
            "_________________________________________________________________\n",
            "(60000, 28, 28) (10000, 28, 28)\n",
            "Epoch 1/20\n",
            "235/235 [==============================] - 134s 565ms/step - loss: 0.4854 - val_loss: 0.4659\n",
            "Epoch 2/20\n",
            "235/235 [==============================] - 124s 530ms/step - loss: 0.3678 - val_loss: 0.3517\n",
            "Epoch 3/20\n",
            "235/235 [==============================] - 126s 535ms/step - loss: 0.3024 - val_loss: 0.2899\n",
            "Epoch 4/20\n",
            "235/235 [==============================] - 128s 546ms/step - loss: 0.2551 - val_loss: 0.2494\n",
            "Epoch 5/20\n",
            "235/235 [==============================] - 129s 549ms/step - loss: 0.2197 - val_loss: 0.2152\n",
            "Epoch 6/20\n",
            "235/235 [==============================] - 126s 538ms/step - loss: 0.1926 - val_loss: 0.1887\n",
            "Epoch 7/20\n",
            "235/235 [==============================] - 124s 526ms/step - loss: 0.1715 - val_loss: 0.1685\n",
            "Epoch 8/20\n",
            "122/235 [==============>...............] - ETA: 57s - loss: 0.1578"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Conv2D, Activation, MaxPool2D\n",
        "from tensorflow.keras.layers import BatchNormalization, Flatten, Reshape, Conv2DTranspose, LeakyReLU\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "print(\"\")\n",
        "\n",
        "## Seeding\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "## Loading the dataset and then normalizing the images.\n",
        "# dataset = tf.keras.datasets.fashion_mnist\n",
        "dataset = tf.keras.datasets.mnist\n",
        "(x_train, y_train),(x_test, y_test) = dataset.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "## Hyperparameters\n",
        "H = 28\n",
        "W = 28\n",
        "C = 1\n",
        "\n",
        "## Latent space\n",
        "latent_dim = 128\n",
        "\n",
        "## Building the autoencoder\n",
        "inputs = Input(shape=(H, W, C), name=\"inputs\")\n",
        "x = inputs\n",
        "\n",
        "x = Conv2D(32, (3, 3), padding=\"same\")(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = LeakyReLU(alpha=0.2)(x)\n",
        "x = MaxPool2D((2, 2))(x)\n",
        "\n",
        "x = Conv2D(64, (3, 3), padding=\"same\")(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = LeakyReLU(alpha=0.2)(x)\n",
        "x = MaxPool2D((2, 2))(x)\n",
        "\n",
        "x = Flatten()(x)\n",
        "units = x.shape[1]\n",
        "x = Dense(latent_dim, name=\"latent\")(x)\n",
        "x = Dense(units)(x)\n",
        "x = LeakyReLU(alpha=0.2)(x)\n",
        "x = Reshape((7, 7, 64))(x)\n",
        "\n",
        "x = Conv2DTranspose(64, (3, 3), strides=2, padding=\"same\")(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = LeakyReLU(alpha=0.2)(x)\n",
        "\n",
        "x = Conv2DTranspose(1, (3, 3), strides=2, padding=\"same\")(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation(\"sigmoid\", name=\"outputs\")(x)\n",
        "\n",
        "outputs = x\n",
        "\n",
        "autoencoder = Model(inputs, outputs)\n",
        "autoencoder.compile(optimizer=Adam(1e-3), loss='binary_crossentropy')\n",
        "autoencoder.summary()\n",
        "\n",
        "print(x_train.shape, x_test.shape)\n",
        "\n",
        "autoencoder.fit(\n",
        "    x_train,\n",
        "    x_train,\n",
        "    epochs=20,\n",
        "    batch_size=256,\n",
        "    shuffle=False,\n",
        "    validation_data=(x_test, x_test)\n",
        ")\n",
        "\n",
        "test_pred_y = autoencoder.predict(x_test)\n",
        "\n",
        "n = 10  ## how many digits we will display\n",
        "plt.figure(figsize=(20, 4))\n",
        "for i in range(n):\n",
        "    ## display original\n",
        "    ax = plt.subplot(2, n, i + 1)\n",
        "    ax.set_title(\"Original Image\")\n",
        "    plt.imshow(x_test[i].reshape(H, W))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    ## display reconstruction\n",
        "    ax = plt.subplot(2, n, i + 1 + n)\n",
        "    ax.set_title(\"Predicted Image\")\n",
        "    plt.imshow(test_pred_y[i].reshape(H, W))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "plt.savefig(\"res/convolutional_autoencoder.png\")"
      ]
    }
  ]
}